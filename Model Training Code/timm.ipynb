{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing necessary libraries"
      ],
      "metadata": {
        "id": "Q0UrElXSYNAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MR7SK91xMNOT",
        "outputId": "ce486a15-b71f-4107-b253-4b08f8252292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.18.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.23.5)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->timm) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from timm.data.transforms_factory import create_transform\n",
        "import timm\n",
        "from PIL import Image\n",
        "import timm\n",
        "import timm.data.mixup as mixup\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from timm.data import create_transform\n",
        "from timm.utils import ModelEma\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import time, copy"
      ],
      "metadata": {
        "id": "9d7m_63BvuwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting to Google Drive"
      ],
      "metadata": {
        "id": "Fjso5IQfYRLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sg8a0Vz7vuyi",
        "outputId": "22a06236-e92b-4541-d4bb-e89bf20e9bd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the pretrained model"
      ],
      "metadata": {
        "id": "HPWeHTWfYTuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model\n",
        "model = timm.create_model('tf_efficientnet_b7', pretrained = True, drop_rate = 0.5, num_classes = 3)"
      ],
      "metadata": {
        "id": "g6-f4dbVvu0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dblyAovfZMeH",
        "outputId": "1fcf5f06-2c56-4ac2-d089-e59f7b99a284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Linear(in_features=2560, out_features=3, bias=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding additional linear layers at the end"
      ],
      "metadata": {
        "id": "F5mLGCXYYXUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features = 2560, out_features=2560, bias = True),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features = 2560, out_features=2560, bias = True),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features = 2560, out_features=2560, bias = True),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features = 2560, out_features=2560, bias = True),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features = 2560, out_features=2560, bias = True),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features = 2560, out_features=2560, bias = True),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features = 2560, out_features=2560, bias = True),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(in_features = 2560, out_features=3, bias = True),\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "TDRhy5QdZWM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aPjiM9jSjEh",
        "outputId": "074788c3-f659-4a88-d1f8-d083fcb6e580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "  (5): ReLU()\n",
              "  (6): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "  (7): ReLU()\n",
              "  (8): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "  (9): ReLU()\n",
              "  (10): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "  (11): ReLU()\n",
              "  (12): Linear(in_features=2560, out_features=2560, bias=True)\n",
              "  (13): ReLU()\n",
              "  (14): Linear(in_features=2560, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Image transforms ready"
      ],
      "metadata": {
        "id": "bIy2Hu7kYfCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.pretrained_cfg\n",
        "\n",
        "timm.data.resolve_data_config(model.pretrained_cfg)\n",
        "\n",
        "data_cfg = timm.data.resolve_data_config(model.pretrained_cfg)\n",
        "transform = timm.data.create_transform(**data_cfg, is_training = True, auto_augment='rand-m9-mstd0.5-inc1')\n",
        "transform.transforms[0] = transforms.Resize((600, 600))\n",
        "transform.transforms.pop(1)\n",
        "transform.transforms[1].ops = transform.transforms[1].ops[:-2]\n",
        "transform\n",
        "\n",
        "# transform.transforms[1]\n",
        "\n",
        "# valid_transform = transforms.Compose([\n",
        "#         transforms.Resize((600, 600)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "#     ])\n",
        "\n",
        "# # Load your dataset\n",
        "# train_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Dog_X_ray/Dog_heart/Train', transform=transform)\n",
        "# val_dataset = datasets.ImageFolder(root='/content/drive/MyDrive/Dog_X_ray/Dog_heart/Valid', transform=valid_transform)\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upPePuWAGeqA",
        "outputId": "cac6928b-9288-487d-9d92-9f5a534e5e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=(600, 600), interpolation=bilinear, max_size=None, antialias=True)\n",
              "    RandAugment(n=2, ops=\n",
              "\tAugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)\n",
              "\tAugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5))\n",
              "    MaybeToTensor()\n",
              "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloaders = {'Train': train_loader, 'Valid': val_loader}\n",
        "# dataset_sizes = {'Train': len(train_dataset), 'Valid': len(val_dataset)}\n",
        "# class_names = train_dataset.classes\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "urpUqrN6nQza",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfreezing specific layers"
      ],
      "metadata": {
        "id": "RjQQrJuxYjaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "for param in model.blocks[-3].parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "for param in model.blocks[-2].parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "for param in model.blocks[-1].parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "for param in model.conv_head.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "for param in model.classifier.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, threshold=0.0001, threshold_mode='abs')"
      ],
      "metadata": {
        "id": "JI4QBSHUvu23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Function"
      ],
      "metadata": {
        "id": "DkHG_ujpYmBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['Train', 'Valid']:\n",
        "            if phase == 'Train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'Train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'Train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'Train':\n",
        "                scheduler.step(loss)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'Valid' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "metadata": {
        "id": "7iU9kh5kbjut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import copy\n",
        "# import time\n",
        "# import os\n",
        "# import shutil\n",
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data import DataLoader, Subset\n",
        "# from sklearn.model_selection import KFold\n",
        "# import torch\n",
        "\n",
        "# # Define the path to the merged folder\n",
        "# merged_folder = '/content/drive/MyDrive/Dog_X_ray/Dog_heart/All'\n",
        "\n",
        "# # Create the merged folder if it doesn't exist\n",
        "# if not os.path.exists(merged_folder):\n",
        "#     os.makedirs(merged_folder)\n",
        "\n",
        "# # Paths to the original folders\n",
        "# train_folder = '/content/drive/MyDrive/Dog_X_ray/Dog_heart/Train'\n",
        "# valid_folder = '/content/drive/MyDrive/Dog_X_ray/Dog_heart/Valid'\n",
        "\n",
        "# # Merge the Train and Valid folders into the All folder\n",
        "# for folder in [train_folder, valid_folder]:\n",
        "#     for class_name in os.listdir(folder):\n",
        "#         class_folder = os.path.join(folder, class_name)\n",
        "#         merged_class_folder = os.path.join(merged_folder, class_name)\n",
        "\n",
        "#         if not os.path.exists(merged_class_folder):\n",
        "#             os.makedirs(merged_class_folder)\n",
        "\n",
        "#         for file_name in os.listdir(class_folder):\n",
        "#             file_path = os.path.join(class_folder, file_name)\n",
        "#             shutil.copy(file_path, merged_class_folder)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Bl7-syLXLGr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "# Define the path to the merged folder\n",
        "merged_folder = '/content/drive/MyDrive/Dog_X_ray/Dog_heart/All/'\n",
        "\n",
        "# Load the merged dataset\n",
        "dataset = datasets.ImageFolder(root=merged_folder, transform=transform)\n",
        "\n",
        "# Initialize KFold\n",
        "k_folds = 7\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "FpqQOrhzNhOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLL8l3r_eT_H",
        "outputId": "32616df5-a8c0-489e-8a51-9abfde038052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('/content/drive/MyDrive/model_fold3_7.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2MMrpkk2OT7",
        "outputId": "d6d82146-d291-40f6-ed75-0836a09a1cfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform k-fold cross-validation\n",
        "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
        "    print(f'Fold {fold}')\n",
        "\n",
        "    # Sample elements randomly from a given list of ids, no replacement\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
        "\n",
        "    # Define data loaders for training and validation\n",
        "    train_loader = DataLoader(dataset, batch_size=16, sampler=train_subsampler)\n",
        "    val_loader = DataLoader(dataset, batch_size=16, sampler=val_subsampler)\n",
        "\n",
        "    dataloaders = {'Train': train_loader, 'Valid': val_loader}\n",
        "    dataset_sizes = {'Train': len(train_subsampler), 'Valid': len(val_subsampler)}\n",
        "    class_names = dataset.classes\n",
        "    # Train and validate your model\n",
        "    model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=6)\n",
        "\n",
        "    # Save the best model weights for this fold\n",
        "    torch.save(model.state_dict(), f'/content/drive/MyDrive/model_foldlm_{fold}.pth')"
      ],
      "metadata": {
        "id": "Za1Qiz-XN8ke",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ec79432-60f9-4ce5-c81a-a2266bdd6c92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 0\n",
            "Epoch 0/5\n",
            "----------\n",
            "Train Loss: 1.0918 Acc: 0.4553\n",
            "Valid Loss: 0.7346 Acc: 0.5791\n",
            "\n",
            "Epoch 1/5\n",
            "----------\n",
            "Train Loss: 0.8641 Acc: 0.5574\n",
            "Valid Loss: 0.8069 Acc: 0.4640\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train Loss: 0.9030 Acc: 0.5225\n",
            "Valid Loss: 1.1124 Acc: 0.5504\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train Loss: 0.9773 Acc: 0.4390\n",
            "Valid Loss: 1.1123 Acc: 0.4173\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train Loss: 0.8643 Acc: 0.5261\n",
            "Valid Loss: 1.0093 Acc: 0.4676\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train Loss: 0.8220 Acc: 0.5471\n",
            "Valid Loss: 0.8774 Acc: 0.4964\n",
            "\n",
            "Training complete in 21m 53s\n",
            "Best val Acc: 0.579137\n",
            "Fold 1\n",
            "Epoch 0/5\n",
            "----------\n",
            "Train Loss: 1.0354 Acc: 0.4420\n",
            "Valid Loss: 3.7543 Acc: 0.3381\n",
            "\n",
            "Epoch 1/5\n",
            "----------\n",
            "Train Loss: 1.0022 Acc: 0.4456\n",
            "Valid Loss: 0.9428 Acc: 0.5072\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train Loss: 0.9801 Acc: 0.4709\n",
            "Valid Loss: 0.9376 Acc: 0.5432\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train Loss: 0.8783 Acc: 0.5177\n",
            "Valid Loss: 0.7779 Acc: 0.6331\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train Loss: 0.8092 Acc: 0.5604\n",
            "Valid Loss: 0.8278 Acc: 0.6043\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train Loss: 0.9198 Acc: 0.4937\n",
            "Valid Loss: 0.8087 Acc: 0.5863\n",
            "\n",
            "Training complete in 21m 50s\n",
            "Best val Acc: 0.633094\n",
            "Fold 2\n",
            "Epoch 0/5\n",
            "----------\n",
            "Train Loss: 0.8016 Acc: 0.5772\n",
            "Valid Loss: 0.8071 Acc: 0.4964\n",
            "\n",
            "Epoch 1/5\n",
            "----------\n",
            "Train Loss: 0.8326 Acc: 0.5502\n",
            "Valid Loss: 0.7958 Acc: 0.5432\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train Loss: 0.8008 Acc: 0.5892\n",
            "Valid Loss: 0.7903 Acc: 0.5755\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n",
            "Train Loss: 0.7692 Acc: 0.5718\n",
            "Valid Loss: 0.7778 Acc: 0.6007\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n",
            "Train Loss: 0.8057 Acc: 0.5664\n",
            "Valid Loss: 0.9214 Acc: 0.5108\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n",
            "Train Loss: 0.7626 Acc: 0.5808\n",
            "Valid Loss: 0.8058 Acc: 0.5755\n",
            "\n",
            "Training complete in 21m 51s\n",
            "Best val Acc: 0.600719\n",
            "Fold 3\n",
            "Epoch 0/5\n",
            "----------\n",
            "Train Loss: 0.7164 Acc: 0.6126\n",
            "Valid Loss: 0.6860 Acc: 0.6331\n",
            "\n",
            "Epoch 1/5\n",
            "----------\n",
            "Train Loss: 0.7177 Acc: 0.6048\n",
            "Valid Loss: 0.6938 Acc: 0.6259\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n",
            "Train Loss: 0.6928 Acc: 0.6204\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-030e123828bc>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mclass_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Train and validate your model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Save the best model weights for this fold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-122515303d6a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                 \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6s9vQ219N8oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "model_name = 'model_fold3_7'\n",
        "\n",
        "model.load_state_dict(torch.load(f'/content/drive/MyDrive/{model_name}.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Define a function to make predictions on a single image\n",
        "def predict_image(image_path):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((600, 600)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img_tensor = transform(img)\n",
        "    img_tensor = img_tensor.unsqueeze(0)\n",
        "    img_tensor = img_tensor.cuda()\n",
        "    output = model(img_tensor)\n",
        "    _, predicted = torch.max(output, 1)\n",
        "    return predicted.item()\n",
        "\n",
        "paths = []\n",
        "answers = []\n",
        "image_folder = '/content/drive/MyDrive/Dog_X_ray/Test/'\n",
        "filenames = os.listdir(image_folder)\n",
        "for filename in os.listdir(image_folder):\n",
        "    image_path = os.path.join(image_folder, filename)\n",
        "    prediction = predict_image(image_path)\n",
        "\n",
        "    paths.append(image_path)\n",
        "    answers.append(prediction)\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({filenames[0]: filenames[1:], answers[0]: answers[1:]})\n",
        "df\n",
        "\n",
        "df.to_csv(f'/content/drive/MyDrive/{model_name}.csv', index=False)"
      ],
      "metadata": {
        "id": "F1VybBCtN8sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p59PH7auIi_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5DyJnCE8ofxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}